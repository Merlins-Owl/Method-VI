# FIX-021: Add Temperature Control to LLM API Calls

**Priority:** Critical
**Status:** ‚úÖ Implemented
**Date:** 2025-12-31

---

## Problem Statement

All LLM-based metric calculations (CI, EV, IAS, EFI, PCI) were using Claude's default temperature (~1.0), causing significant variance in scores between runs. The same content produced:
- Test Run 5: CI = 0.60
- Test Run 7: CI = 0.30
- **Variance: 50% score change** with identical input

This violated the governance requirement for deterministic, reproducible metrics.

---

## Root Cause

The `ClaudeRequest` struct in `anthropic.rs` did not include a `temperature` field, and the `call_claude()` function signature did not accept a temperature parameter. As a result, all API calls used Claude's default temperature (~1.0), which maximizes creativity at the cost of consistency.

For governance metrics, this high variability is unacceptable. The same content should yield the same scores to ensure fair, reproducible evaluation.

---

## Changes Implemented

### 1. Updated `ClaudeRequest` Struct

**File:** `src-tauri/src/api/anthropic.rs:34-44`

**Before:**
```rust
#[derive(Debug, Serialize)]
struct ClaudeRequest {
    model: String,
    max_tokens: u32,
    messages: Vec<Message>,
    #[serde(skip_serializing_if = "Option::is_none")]
    system: Option<String>,
}
```

**After:**
```rust
#[derive(Debug, Serialize)]
struct ClaudeRequest {
    model: String,
    max_tokens: u32,
    messages: Vec<Message>,
    #[serde(skip_serializing_if = "Option::is_none")]
    system: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,  // ‚Üê ADDED
}
```

---

### 2. Updated `call_claude()` Function Signature

**File:** `src-tauri/src/api/anthropic.rs:110-140`

**Before:**
```rust
pub async fn call_claude(
    &self,
    system_prompt: &str,
    user_message: &str,
    model: Option<&str>,
    max_tokens: Option<u32>,
) -> Result<String>
```

**After:**
```rust
pub async fn call_claude(
    &self,
    system_prompt: &str,
    user_message: &str,
    model: Option<&str>,
    max_tokens: Option<u32>,
    temperature: Option<f32>,  // ‚Üê ADDED
) -> Result<String>
```

**Request Construction:**
```rust
let request_body = ClaudeRequest {
    model: model.to_string(),
    max_tokens,
    messages: vec![Message { ... }],
    system: if system_prompt.is_empty() { None } else { Some(...) },
    temperature,  // ‚Üê ADDED
};
```

---

### 3. Updated Governance Metric Call Sites (Temperature = 0.0)

**File:** `src-tauri/src/agents/governance_telemetry.rs`

All critical metrics now use **temperature=0.0** for deterministic results:

| Metric | Function | Line | Temperature |
|--------|----------|------|-------------|
| **CI** | `calculate_ci()` | 260 | `Some(0.0)` |
| **EV** | `calculate_entropy()` | 326 | `Some(0.0)` |
| **IAS** | `calculate_ias()` | 461 | `Some(0.0)` |
| **EFI** | `calculate_efi()` | 522 | `Some(0.0)` |
| **PCI** | `calculate_pci()` | 657 | `Some(0.0)` |
| Governance Calibration | `generate_governance_calibration()` | 1177 | `Some(0.0)` |
| Relevance Check | `check_synthesis_relevance()` | 1463 | `Some(0.0)` |

**Total:** 7 governance-critical call sites updated

**Example Change:**
```rust
// Before
let response = self.api_client
    .call_claude(system_prompt, &user_message, None, Some(1024))
    .await?;

// After
let response = self.api_client
    .call_claude(system_prompt, &user_message, None, Some(1024), Some(0.0))
    .await?;
```

---

### 4. Updated Other Agent Call Sites (Temperature = None)

For backward compatibility and to preserve creative behavior where desired, all non-governance agents pass `None` for temperature (uses Claude's default):

| Agent | File | Call Sites Updated |
|-------|------|-------------------|
| Validation & Learning | `validation_learning.rs` | 6 |
| Analysis & Synthesis | `analysis_synthesis.rs` | 13 |
| Structure & Redesign | `structure_redesign.rs` | 2 |
| Scope & Pattern | `scope_pattern.rs` | 3 |

**Total:** 24 call sites updated for backward compatibility

**Example:**
```rust
let response = self.api_client
    .call_claude(system_prompt, &user_message, None, Some(4096), None)
    //                                                           ^^^^ Uses Claude's default
    .await?;
```

---

## Verification

### Compilation Status
‚úÖ **PASSED** - Code compiles successfully with no errors

```bash
$ cargo check
    Checking method-vi v0.1.0
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 1.09s
```

### Test Suite Created

**File:** `tests/test_temperature_determinism.rs`

Two new tests verify deterministic behavior:

1. **`test_ci_determinism`**
   - Runs CI calculation 3 times with identical content
   - Verifies variance ‚â§ 0.02 (2% tolerance)
   - Run with: `cargo test test_ci_determinism -- --ignored`

2. **`test_all_metrics_determinism`**
   - Tests CI, IAS, EFI across 2 runs
   - Verifies all metrics are deterministic
   - Run with: `cargo test test_all_metrics_determinism -- --ignored`

### Manual Verification Checklist

- [x] ClaudeRequest struct has temperature field
- [x] call_claude signature accepts temperature parameter
- [x] Temperature included in API request body
- [x] CI calculation uses temperature=0.0
- [x] EV (entropy) calculation uses temperature=0.0
- [x] IAS calculation uses temperature=0.0
- [x] EFI calculation uses temperature=0.0
- [x] PCI calculation uses temperature=0.0
- [x] All non-governance calls use temperature=None
- [x] Code compiles without errors
- [x] Test suite created for verification

---

## Impact

### Before FIX-021
- **CI variance:** 0.60 ‚Üí 0.30 (50% change)
- Same content could trigger different HALT decisions
- Governance metrics unreliable and non-reproducible
- Test runs not comparable

### After FIX-021
- **CI variance:** Expected ‚â§ 0.02 (2% tolerance)
- Same content produces same scores consistently
- HALT decisions are deterministic and fair
- Test runs are reproducible and comparable

---

## Technical Details

### Why Temperature=0.0?

Temperature controls randomness in LLM responses:
- **0.0** = Deterministic (always picks most likely token)
- **1.0** = Creative (samples from probability distribution)
- **>1.0** = Highly creative (more random)

For governance metrics, we need:
- ‚úÖ Same input ‚Üí Same output (deterministic)
- ‚úÖ Reproducible across runs
- ‚úÖ Fair evaluation (content quality determines score, not randomness)

### Why None for Other Agents?

Non-governance agents (analysis, synthesis, validation) benefit from creativity:
- Richer diagnostic narratives
- More varied lens perspectives
- Better synthesis insights

Using `None` (Claude's default ~1.0) preserves this creative behavior.

---

## Related Issues

- **Test Run #7 Analysis:** Identified CI variance as root cause of confusion
- **Metrics Audit Report:** Documented temperature as CRITICAL Priority 1 fix
- **FIX-009:** Step-aware metrics (now deterministic within each step)
- **FIX-017:** EV disabled (still calculated deterministically for future use)

---

## Git Commit

```bash
git add method-vi/src-tauri/src/api/anthropic.rs
git add method-vi/src-tauri/src/agents/governance_telemetry.rs
git add method-vi/src-tauri/src/agents/*.rs
git add method-vi/src-tauri/tests/test_temperature_determinism.rs
git commit -m "fix: FIX-021 - Add temperature control to LLM API calls

PROBLEM:
LLM-based metrics (CI, EV, IAS, EFI, PCI) showed high variance between
runs due to using Claude's default temperature (~1.0). Same content
produced CI: 0.60 in Test 5 but CI: 0.30 in Test 7 (50% variance).

ROOT CAUSE:
ClaudeRequest struct and call_claude() function did not support
temperature parameter. All API calls defaulted to temperature ~1.0,
maximizing creativity at the cost of consistency.

FIX:
1. Added temperature field to ClaudeRequest struct (anthropic.rs:43)
2. Updated call_claude() signature to accept temperature parameter
3. Set temperature=0.0 for all governance metric calculations:
   - CI (governance_telemetry.rs:260)
   - EV/Entropy (governance_telemetry.rs:326)
   - IAS (governance_telemetry.rs:461)
   - EFI (governance_telemetry.rs:522)
   - PCI (governance_telemetry.rs:657)
   - Governance Calibration (governance_telemetry.rs:1177)
   - Relevance Check (governance_telemetry.rs:1463)
4. Updated all other call sites with temperature=None for backward
   compatibility (24 call sites across 4 agent files)

IMPACT:
- Governance metrics now deterministic (same content ‚Üí same score)
- Expected variance reduced from 50% to ‚â§2%
- HALT decisions fair and reproducible
- Test runs comparable

VERIFICATION:
- ‚úÖ Code compiles successfully
- ‚úÖ Created test_temperature_determinism.rs test suite
- ‚úÖ 7 governance call sites use temperature=0.0
- ‚úÖ 24 other call sites use temperature=None

RELATED:
- Metrics System Audit Report (Priority 1: CRITICAL)
- Test Run #7 HALT Analysis

ü§ñ Generated with Claude Code

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Next Steps

1. **Run Verification Tests:**
   ```bash
   # Requires ANTHROPIC_API_KEY environment variable
   set ANTHROPIC_API_KEY=your-key-here
   cargo test test_ci_determinism -- --ignored
   cargo test test_all_metrics_determinism -- --ignored
   ```

2. **Run Full Test Run #8:**
   - Use same content as Test Run #7
   - Verify CI score is now consistent
   - Confirm HALT conditions are deterministic

3. **Compare Results:**
   - Test Run #7 (before): CI = 0.30 (with variance)
   - Test Run #8 (after): CI = ??? (should be consistent across runs)

4. **Document Baseline:**
   - Record expected CI/IAS/EFI values for standard test content
   - Use as regression test baseline for future changes

---

**Implementation Complete:** ‚úÖ
**Code Status:** Compiled and Ready
**Testing Status:** Test Suite Created (Requires API Key)
**Documentation Status:** Complete
